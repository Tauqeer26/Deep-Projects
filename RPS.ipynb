{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    " \n",
    "\n",
    "import os\n",
    "import sys\n",
    "import PIL.Image\n",
    "import scipy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training rock images: 840\n",
      "total training paper images: 840\n",
      "total training scissors images: 840\n",
      "['rock01-000.png', 'rock01-001.png', 'rock01-002.png', 'rock01-003.png', 'rock01-004.png', 'rock01-005.png', 'rock01-006.png', 'rock01-007.png', 'rock01-008.png', 'rock01-009.png']\n",
      "['paper01-000.png', 'paper01-001.png', 'paper01-002.png', 'paper01-003.png', 'paper01-004.png', 'paper01-005.png', 'paper01-006.png', 'paper01-007.png', 'paper01-008.png', 'paper01-009.png']\n",
      "['scissors01-000.png', 'scissors01-001.png', 'scissors01-002.png', 'scissors01-003.png', 'scissors01-004.png', 'scissors01-005.png', 'scissors01-006.png', 'scissors01-007.png', 'scissors01-008.png', 'scissors01-009.png']\n"
     ]
    }
   ],
   "source": [
    "rock_dir = os.path.join(\"F:/Dataset/rps/rps/rock\")\n",
    "paper_dir = os.path.join(\"F:/Dataset/rps/rps/paper\")\n",
    "scissors_dir = os.path.join(\"F:/Dataset/rps/rps/scissors\")\n",
    "\n",
    "print('total training rock images:', len(os.listdir(rock_dir)))\n",
    "print('total training paper images:', len(os.listdir(paper_dir)))\n",
    "print('total training scissors images:', len(os.listdir(scissors_dir)))\n",
    "\n",
    "rock_files = os.listdir(rock_dir)\n",
    "print(rock_files[:10])\n",
    "\n",
    "paper_files = os.listdir(paper_dir)\n",
    "print(paper_files[:10])\n",
    "\n",
    "scissors_files = os.listdir(scissors_dir)\n",
    "print(scissors_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "840"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2520 images belonging to 3 classes.\n",
      "Found 372 images belonging to 3 classes.\n",
      "WARNING:tensorflow:From f:\\Users\\KHAN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 3,473,475\n",
      "Trainable params: 3,473,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From f:\\Users\\KHAN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.8324 - acc: 0.5833\n",
      "79/79 [==============================] - 459s 6s/step - loss: 1.1002 - acc: 0.3933 - val_loss: 0.8324 - val_acc: 0.5833\n",
      "Epoch 2/15\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.2897 - acc: 0.9677\n",
      "79/79 [==============================] - 400s 5s/step - loss: 0.8060 - acc: 0.6329 - val_loss: 0.2897 - val_acc: 0.9677\n",
      "Epoch 3/15\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.3046 - acc: 0.8387\n",
      "79/79 [==============================] - 388s 5s/step - loss: 0.5014 - acc: 0.7841 - val_loss: 0.3046 - val_acc: 0.8387\n",
      "Epoch 4/15\n",
      "12/12 [==============================] - 25s 2s/step - loss: 0.0372 - acc: 1.0000\n",
      "79/79 [==============================] - 405s 5s/step - loss: 0.3150 - acc: 0.8794 - val_loss: 0.0372 - val_acc: 1.0000\n",
      "Epoch 5/15\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.0846 - acc: 0.9570\n",
      "79/79 [==============================] - 399s 5s/step - loss: 0.2253 - acc: 0.9175 - val_loss: 0.0846 - val_acc: 0.9570\n",
      "Epoch 6/15\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.0882 - acc: 0.9570\n",
      "79/79 [==============================] - 400s 5s/step - loss: 0.1779 - acc: 0.9341 - val_loss: 0.0882 - val_acc: 0.9570\n",
      "Epoch 7/15\n",
      "12/12 [==============================] - 22s 2s/step - loss: 0.0937 - acc: 0.9543\n",
      "79/79 [==============================] - 3451s 44s/step - loss: 0.1392 - acc: 0.9552 - val_loss: 0.0937 - val_acc: 0.9543\n",
      "Epoch 8/15\n",
      "12/12 [==============================] - 22s 2s/step - loss: 1.9370 - acc: 0.6129\n",
      "79/79 [==============================] - 423s 5s/step - loss: 0.1096 - acc: 0.9639 - val_loss: 1.9370 - val_acc: 0.6129\n",
      "Epoch 9/15\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.1530 - acc: 0.9597\n",
      "79/79 [==============================] - 414s 5s/step - loss: 0.1289 - acc: 0.9560 - val_loss: 0.1530 - val_acc: 0.9597\n",
      "Epoch 10/15\n",
      "12/12 [==============================] - 20s 2s/step - loss: 0.1717 - acc: 0.9435\n",
      "79/79 [==============================] - 37001s 468s/step - loss: 0.1295 - acc: 0.9607 - val_loss: 0.1717 - val_acc: 0.9435\n",
      "Epoch 11/15\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.0920 - acc: 0.9543\n",
      "79/79 [==============================] - 391s 5s/step - loss: 0.0802 - acc: 0.9746 - val_loss: 0.0920 - val_acc: 0.9543\n",
      "Epoch 12/15\n",
      "12/12 [==============================] - 19s 2s/step - loss: 0.0722 - acc: 0.9785\n",
      "79/79 [==============================] - 379s 5s/step - loss: 0.0779 - acc: 0.9782 - val_loss: 0.0722 - val_acc: 0.9785\n",
      "Epoch 13/15\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.0859 - acc: 0.9516\n",
      "79/79 [==============================] - 374s 5s/step - loss: 0.1154 - acc: 0.9643 - val_loss: 0.0859 - val_acc: 0.9516\n",
      "Epoch 14/15\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.0442 - acc: 0.9785\n",
      "79/79 [==============================] - 367s 5s/step - loss: 0.0677 - acc: 0.9833 - val_loss: 0.0442 - val_acc: 0.9785\n",
      "Epoch 15/15\n",
      "12/12 [==============================] - 18s 2s/step - loss: 0.0693 - acc: 0.9785\n",
      "79/79 [==============================] - 367s 5s/step - loss: 0.0647 - acc: 0.9786 - val_loss: 0.0693 - val_acc: 0.9785\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "TRAINING_DIR = \"F:/Dataset/rps/rps/\"\n",
    "training_datagen = ImageDataGenerator(\n",
    "      rescale = 1./255,\n",
    "\t  rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "VALIDATION_DIR = \"F:/Dataset/rps-test-set/rps-test-set\"\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "\tTRAINING_DIR,\n",
    "\ttarget_size=(150,150),\n",
    "\tclass_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "\tVALIDATION_DIR,\n",
    "\ttarget_size=(150,150),\n",
    "\tclass_mode='categorical'\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    #tf.keras.layers.Dropout(0.5),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit_generator(train_generator, epochs=15, validation_data = validation_generator, verbose = 1)\n",
    "\n",
    "model.save(\"rps.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "history = model.fit_generator(train_generator, epochs=25, validation_data = validation_generator, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "model_trained=tf.keras.models.load_model('rps.h5')\n",
    "file=\"F:/testing\"\n",
    "test_batches=ImageDataGenerator(rescale = 1./255).flow_from_directory(file,target_size=(150,150),class_mode='categorical')\n",
    "#model_trained.predict(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9999988e-01, 3.2222400e-14, 1.2364332e-07],\n",
       "       [8.3996873e-08, 1.9106050e-09, 9.9999988e-01],\n",
       "       [3.5382472e-12, 1.0000000e+00, 5.1730955e-14],\n",
       "       [2.2061781e-07, 1.9906938e-09, 9.9999976e-01],\n",
       "       [4.7622951e-08, 1.2528550e-09, 1.0000000e+00],\n",
       "       [3.5446512e-08, 1.1885029e-09, 1.0000000e+00],\n",
       "       [1.0000000e+00, 1.5623517e-18, 2.3305578e-11],\n",
       "       [7.4608828e-08, 5.7958496e-09, 9.9999988e-01],\n",
       "       [1.2684377e-10, 1.0000000e+00, 2.0229803e-13],\n",
       "       [1.0000000e+00, 1.9833639e-17, 5.7962568e-10],\n",
       "       [3.8681601e-12, 1.0000000e+00, 5.6658210e-14],\n",
       "       [1.0000000e+00, 8.3096639e-17, 6.6486994e-10],\n",
       "       [1.4799446e-10, 1.0000000e+00, 4.2501237e-13],\n",
       "       [3.1730835e-10, 1.0000000e+00, 3.6168178e-12],\n",
       "       [6.3720344e-12, 1.0000000e+00, 5.1623914e-14],\n",
       "       [2.5256599e-11, 1.0000000e+00, 6.5252518e-14],\n",
       "       [3.3702744e-08, 6.5999073e-09, 1.0000000e+00],\n",
       "       [1.1733352e-09, 1.0000000e+00, 6.9876765e-13],\n",
       "       [1.7973190e-07, 2.1644995e-09, 9.9999976e-01],\n",
       "       [6.3808280e-08, 5.2196119e-09, 9.9999988e-01],\n",
       "       [1.0000000e+00, 4.3171541e-16, 3.4002063e-09],\n",
       "       [1.0000000e+00, 1.1992975e-15, 9.4019974e-09],\n",
       "       [1.0000000e+00, 3.7258684e-15, 1.7873090e-08],\n",
       "       [1.0000000e+00, 3.7736670e-18, 8.3307486e-11]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trained.predict_generator(test_batches,steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-7b426e99ae18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_imgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plots' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
